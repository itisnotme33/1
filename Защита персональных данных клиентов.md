
<h1>Содержание<span class="tocSkip"></span></h1>
<div class="toc"><ul class="toc-item"><li><span><a href="#Загрузка-данных" data-toc-modified-id="Загрузка-данных-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Загрузка данных</a></span></li><li><span><a href="#Умножение-матриц" data-toc-modified-id="Умножение-матриц-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>Умножение матриц</a></span></li><li><span><a href="#Алгоритм-преобразования" data-toc-modified-id="Алгоритм-преобразования-3"><span class="toc-item-num">3&nbsp;&nbsp;</span>Алгоритм преобразования</a></span></li><li><span><a href="#Проверка-алгоритма" data-toc-modified-id="Проверка-алгоритма-4"><span class="toc-item-num">4&nbsp;&nbsp;</span>Проверка алгоритма</a></span></li><li><span><a href="#Чек-лист-проверки" data-toc-modified-id="Чек-лист-проверки-5"><span class="toc-item-num">5&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>

# Защита персональных данных клиентов

Вам нужно защитить данные клиентов страховой компании «Хоть потоп». Разработайте такой метод преобразования данных, чтобы по ним было сложно восстановить персональную информацию. Обоснуйте корректность его работы.

Нужно защитить данные, чтобы при преобразовании качество моделей машинного обучения не ухудшилось. Подбирать наилучшую модель не требуется.



## Загрузка данных


```python
import pandas as pd
import numpy as np

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
```


```python
data = pd.read_csv('/datasets/insurance.csv')
```


```python
data.info()
data.head()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 5000 entries, 0 to 4999
    Data columns (total 5 columns):
     #   Column             Non-Null Count  Dtype  
    ---  ------             --------------  -----  
     0   Пол                5000 non-null   int64  
     1   Возраст            5000 non-null   float64
     2   Зарплата           5000 non-null   float64
     3   Члены семьи        5000 non-null   int64  
     4   Страховые выплаты  5000 non-null   int64  
    dtypes: float64(2), int64(3)
    memory usage: 195.4 KB






<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Пол</th>
      <th>Возраст</th>
      <th>Зарплата</th>
      <th>Члены семьи</th>
      <th>Страховые выплаты</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>41.0</td>
      <td>49600.0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>46.0</td>
      <td>38000.0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>29.0</td>
      <td>21000.0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>21.0</td>
      <td>41700.0</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>28.0</td>
      <td>26100.0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
data.duplicated().sum()
```




    153



Считаю, дубликаты удалять не стоит. Полное совпадение наших данных вполне реально.


```python
data.isna().sum()
```




    Пол                  0
    Возраст              0
    Зарплата             0
    Члены семьи          0
    Страховые выплаты    0
    dtype: int64




## Умножение матриц



Обозначения:

- $X$ — матрица признаков (нулевой столбец состоит из единиц)

- $y$ — вектор целевого признака

- $P$ — матрица, на которую умножаются признаки

- $w$ — вектор весов линейной регрессии (нулевой элемент равен сдвигу)

Предсказания:

$$
a = Xw
$$

Задача обучения:

$$
w = \arg\min_w MSE(Xw, y)
$$

Формула обучения:

$$
w = (X^T X)^{-1} X^T y
$$

Признаки умножают на обратимую матрицу. Изменится ли качество линейной регрессии?

**Ответ:** Не изменится

**Обоснование:** Докажем, что $ a' = a  $

$$X' = XP$$

$$w' = ((XP)^T (XP))^{-1} (XP)^T y = (P^{T}X^{T}XP)^{-1}P^{T}X^{T}y$$

Далее применим обратное умножение матриц

$$w' = (X^T XP)^{-1} (P^T)^{-1}P^TX^{T}y = (X^{T}XP)^{-1}X^{T}y$$

Еще раз применим обратное умножение матриц

$$w' = P^{-1}(X^{T}X)^{-1}X^{T}y$$

$$X'w' = XPw' = XPP^{-1} (X^TX)^{-1}X^{T} y = X(X^{T}X)^{-1}X^{T}y = Xw$$


Мы выше доказали следующее:

$ a' = a = Xw =XPw' $

Значит, соотношение равно:

$$ w = Pw'$$



## Алгоритм преобразования

**Алгоритм**

 Умножение матрицы признаков на генерируемую случайным образом обратимую матрицу






**Обоснование**

Применение данного алгоритма приведет к перемещению и изменению значений элементов в матрице, что сделает ее неразборчивой и невозможной для чтения без знания обратной матрицы. Таким образом, данные станут непонятными для неавторизованного доступа и будут защищены от кражи, взлома и других видов атак.

## Проверка алгоритма

Запрограммируйте этот алгоритм, применив матричные операции. Проверьте, что качество линейной регрессии из sklearn не отличается до и после преобразования. Примените метрику R2.

До преобразования


```python
features = data.drop('Страховые выплаты',axis=1)
target = data['Страховые выплаты']
```


```python
features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.25, random_state=12345)
```


```python
model = LinearRegression()
model.fit(features_train, target_train)
R2 = r2_score(target_test, model.predict(features_test))
print("R2 =", R2)
```

    R2 = 0.43522757127026546



```python
# Отмасштабированная модель
regressor = LinearRegression()
scaller = StandardScaler()
pipeline = Pipeline([("standard_scaller", scaller),("linear_regression", regressor)])
pipeline.fit(features_train, target_train)
R2_scale = r2_score(target_test, pipeline.predict(features_test))
print("R2 =", R2_scale)
```

    R2 = 0.4352275712702668




После преобразования


```python
random = np.random.normal(size=(4,4))
np.linalg.inv(random) 
features_inv = features @ random
features_inv.head()
```





<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-28460.280420</td>
      <td>-25217.422735</td>
      <td>-8929.300872</td>
      <td>38623.421766</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-21817.167460</td>
      <td>-19323.703824</td>
      <td>-6837.296095</td>
      <td>29606.337575</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-12060.125335</td>
      <td>-10679.758931</td>
      <td>-3777.423861</td>
      <td>16365.131054</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-23915.650112</td>
      <td>-21197.377428</td>
      <td>-7511.545701</td>
      <td>32458.110550</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-14981.587221</td>
      <td>-13271.365335</td>
      <td>-4696.522135</td>
      <td>20330.462054</td>
    </tr>
  </tbody>
</table>
</div>







    

    


```python
features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.25, random_state=12345)
```


```python
model = LinearRegression()
model.fit(features_train, target_train)
R2_new = r2_score(target_test, model.predict(features_test))
print("R2 =", R2_new)
```

    R2 = 0.43522757127026546



```python
regressor = LinearRegression()
scaller = StandardScaler()
pipeline = Pipeline([("standard_scaller", scaller),("linear_regression", regressor)])
pipeline.fit(features_train, target_train)
R2_new_scale = r2_score(target_test, pipeline.predict(features_test))
print("R2 =", R2_new_scale)
```

    R2 = 0.4352275712702668


Вывод: значение R2  не отличается до и после преобразования





  

Итак, мы смогли преобразовать данные таким образом, чтобы было сложно восстановить персональную информацию. При этом качество линейной регресии осталось прежним, о чем нам говорят идентичные значения R2 до и после изменений.



